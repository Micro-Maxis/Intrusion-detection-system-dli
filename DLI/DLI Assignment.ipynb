{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"181bGXD_EYLaTMSMD4CgwgCPen-WNxP5Z","authorship_tag":"ABX9TyPzzBFaQSGwHWAGEHINJYMw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 1. Install dependencies\n"],"metadata":{"id":"VcXeqwX8yUFK"}},{"cell_type":"code","source":["!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.8.0+cpu.html\n"],"metadata":{"id":"t6SOyNFqw-yn","executionInfo":{"status":"ok","timestamp":1756042005367,"user_tz":-480,"elapsed":29696,"user":{"displayName":"Brandon Chong","userId":"05776967428570980977"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0fd4ce9b-c300-408d-a6ea-cb5afeff9ff0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://data.pyg.org/whl/torch-2.8.0+cpu.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcpu/torch_scatter-2.1.2%2Bpt28cpu-cp312-cp312-linux_x86_64.whl (645 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m645.6/645.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcpu/torch_sparse-0.6.18%2Bpt28cpu-cp312-cp312-linux_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcpu/torch_cluster-1.6.3%2Bpt28cpu-cp312-cp312-linux_x86_64.whl (749 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m749.6/749.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt28cpu-cp312-cp312-linux_x86_64.whl (289 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m289.4/289.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-geometric\n","  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.12.15)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.8.3)\n","Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.14.1)\n","Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster, torch-geometric\n","Successfully installed torch-cluster-1.6.3+pt28cpu torch-geometric-2.6.1 torch-scatter-2.1.2+pt28cpu torch-sparse-0.6.18+pt28cpu torch-spline-conv-1.2.2+pt28cpu\n"]}]},{"cell_type":"markdown","source":["# 2. Import packages"],"metadata":{"id":"PhGnhmBa_pJG"}},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/DLI\")\n","\n","import numpy as np\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import MessagePassing\n","from torch_geometric.utils import softmax\n","from torch_geometric.data import Data\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from sklearn.feature_selection import mutual_info_classif\n","from nsl_preprocessing import UniversalNSLKDDPreprocessor\n","from tqdm import tqdm\n"],"metadata":{"id":"V5fZINm52JeS","executionInfo":{"status":"ok","timestamp":1756042042962,"user_tz":-480,"elapsed":37592,"user":{"displayName":"Brandon Chong","userId":"05776967428570980977"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# 3. Data Preprocessing"],"metadata":{"id":"II-qzqnt_0Bz"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VOrMy9fQu-RW","executionInfo":{"status":"ok","timestamp":1756042132124,"user_tz":-480,"elapsed":89163,"user":{"displayName":"Brandon Chong","userId":"05776967428570980977"}},"outputId":"2d0b65e1-7564-430a-aa75-26038a66e5cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== STEP 1: LOADING AND PREPROCESSING DATA ===\n","=== Universal NSL-KDD Preprocessing ===\n","1. Loading and cleaning data...\n","   Train: (125973, 40), Test: (22544, 40)\n","2. Engineering features...\n","   After engineering: (125973, 54)\n","3. Handling outliers...\n","4. Encoding categorical features...\n","5. Scaling features...\n","6. Selecting top 30 features...\n","Selected 30 features using consensus:\n","   1. service_freq\n","   2. flag_freq\n","   3. su_attempted\n","   4. protocol_type\n","   5. service\n","   6. flag\n","   7. src_bytes\n","   8. dst_bytes\n","   9. land\n","  10. wrong_fragment\n","  11. urgent\n","  12. hot\n","  13. num_failed_logins\n","  14. logged_in\n","  15. num_compromised\n","  16. root_shell\n","  17. duration\n","  18. num_root\n","  19. num_file_creations\n","  20. num_shells\n","  21. num_access_files\n","  22. is_guest_login\n","  23. count\n","  24. srv_count\n","  25. serror_rate\n","  26. srv_serror_rate\n","  27. rerror_rate\n","  28. srv_rerror_rate\n","  29. same_srv_rate\n","  30. diff_srv_rate\n","   Final: Train (125973, 30)\n","          Test (22544, 30)\n","   Class balance: Normal 67343, Attack 58630\n","Final data shapes: Train (9999, 24), Test (22544, 24)\n","Class balance: Normal 5345, Attack 4654\n"]}],"source":["class PaperStylePreprocessor:\n","    def __init__(self, target_samples=10000, keep_most_informative=0.8):\n","        self.target_samples = target_samples\n","        self.keep_most_informative = keep_most_informative\n","\n","    def preprocess(self, X_train, y_train, X_test, y_test):\n","        # Convert to numpy\n","        if hasattr(X_train, 'values'):\n","            X_train = X_train.values\n","        if hasattr(y_train, 'values'):\n","            y_train = y_train.values\n","        if hasattr(X_test, 'values'):\n","            X_test = X_test.values\n","        if hasattr(y_test, 'values'):\n","            y_test = y_test.values\n","\n","        # Downsample training data\n","        if len(X_train) > self.target_samples:\n","            unique_classes, counts = np.unique(y_train, return_counts=True)\n","            sample_ratio = self.target_samples / len(X_train)\n","\n","            indices = []\n","            for cls in unique_classes:\n","                cls_indices = np.where(y_train == cls)[0]\n","                n_samples = max(100, int(len(cls_indices) * sample_ratio))\n","                if n_samples <= len(cls_indices):\n","                    selected = np.random.choice(cls_indices, n_samples, replace=False)\n","                else:\n","                    selected = cls_indices\n","                indices.extend(selected)\n","\n","            indices = np.array(indices)\n","            X_train = X_train[indices]\n","            y_train = y_train[indices]\n","\n","        # Keep most informative features\n","        mi_scores = mutual_info_classif(X_train, y_train, random_state=42)\n","        n_keep = int(len(mi_scores) * self.keep_most_informative)\n","        most_informative_idx = np.argsort(mi_scores)[-n_keep:]\n","\n","        X_train = X_train[:, most_informative_idx]\n","        X_test = X_test[:, most_informative_idx]\n","\n","        return X_train, y_train, X_test, y_test\n","\n","# Load and preprocess data\n","print(\"=== STEP 1: LOADING AND PREPROCESSING DATA ===\")\n","train_path = \"KDDTrain+.txt\"\n","test_path = \"KDDTest+.txt\"\n","\n","preprocessor = UniversalNSLKDDPreprocessor(\n","    n_features=30,\n","    sampling_strategy='none',\n","    scaler_type='robust'\n",")\n","X_train, y_train, X_test, y_test = preprocessor.fit_transform(train_path, test_path)\n","\n","# Then: Paper-style preprocessing\n","paper_preprocessor = PaperStylePreprocessor()\n","X_train, y_train, X_test, y_test = paper_preprocessor.preprocess(X_train, y_train, X_test, y_test)\n","\n","print(f\"Final data shapes: Train {X_train.shape}, Test {X_test.shape}\")\n","print(f\"Class balance: Normal {np.sum(y_train==0)}, Attack {np.sum(y_train==1)}\")"]},{"cell_type":"markdown","source":["# 4. Graph Building"],"metadata":{"id":"fZcrnw-g_6VI"}},{"cell_type":"code","source":["def build_graph_fast(X, y, k=15, metric=\"euclidean\"):\n","    if hasattr(X, \"values\"):\n","        X = X.values\n","    if hasattr(y, \"values\"):\n","        y = y.values\n","\n","    n_samples = len(X)\n","    k = min(k, n_samples - 1)\n","\n","    print(f\"Building kNN graph (k={k}) for {n_samples} samples...\")\n","    knn = NearestNeighbors(n_neighbors=k, metric=metric, n_jobs=-1)\n","    knn.fit(X)\n","    dists, indices = knn.kneighbors(X)\n","\n","    # Create edges\n","    src = np.repeat(np.arange(n_samples), k)\n","    dst = indices.flatten()\n","\n","    # Filter valid edges\n","    valid_mask = (src < n_samples) & (dst < n_samples) & (src != dst)\n","    src = src[valid_mask]\n","    dst = dst[valid_mask]\n","\n","    edge_index = torch.tensor(np.vstack([src, dst]), dtype=torch.long)\n","\n","    # RBF edge weights\n","    valid_dists = dists.flatten()[valid_mask]\n","    sigma = np.median(valid_dists[valid_dists > 0]) if len(valid_dists[valid_dists > 0]) > 0 else 1.0\n","    rbf_weights = np.exp(-valid_dists**2 / (2 * sigma**2))\n","    edge_weight = torch.tensor(rbf_weights, dtype=torch.float)\n","\n","    x = torch.tensor(X, dtype=torch.float)\n","    y = torch.tensor(y, dtype=torch.long)\n","\n","    print(f\"Graph built: {x.shape[0]} nodes, {edge_index.shape[1]} edges\")\n","    return Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=y)\n","\n","def apply_graph_augmentation(graph, edge_perturb_rate=0.1, feature_mask_rate=0.2):\n","    num_nodes = graph.x.shape[0]\n","\n","    # Edge perturbation\n","    num_edges = graph.edge_index.shape[1]\n","    num_perturb = int(num_edges * edge_perturb_rate)\n","\n","    if num_perturb > 0:\n","        perturb_idx = torch.randperm(num_edges)[:num_perturb]\n","        perturb_edges = graph.edge_index[:, perturb_idx]\n","\n","        valid_edge_mask = (perturb_edges[0] < num_nodes) & (perturb_edges[1] < num_nodes)\n","        perturb_edges = perturb_edges[:, valid_edge_mask]\n","\n","        if perturb_edges.shape[1] > 0:\n","            new_edge_index = torch.cat([graph.edge_index, perturb_edges], dim=1)\n","            perturb_weights = graph.edge_attr[perturb_idx[valid_edge_mask]]\n","            new_edge_weight = torch.cat([graph.edge_attr, perturb_weights])\n","        else:\n","            new_edge_index = graph.edge_index\n","            new_edge_weight = graph.edge_attr\n","    else:\n","        new_edge_index = graph.edge_index\n","        new_edge_weight = graph.edge_attr\n","\n","    # Feature masking\n","    x_aug = graph.x.clone()\n","    num_features = x_aug.shape[1]\n","    num_mask = int(num_features * feature_mask_rate)\n","\n","    if num_mask > 0:\n","        for i in range(num_nodes):\n","            mask_idx = torch.randperm(num_features)[:num_mask]\n","            x_aug[i, mask_idx] = 0\n","\n","    return Data(x=x_aug, edge_index=new_edge_index, edge_attr=new_edge_weight, y=graph.y)\n","\n","print(\"\\n=== STEP 2: BUILDING GRAPHS ===\")\n","train_graph = build_graph_fast(X_train, y_train, k=15)\n","test_graph = build_graph_fast(X_test, y_test, k=15)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mDwGPQqCXZ8E","executionInfo":{"status":"ok","timestamp":1756042136365,"user_tz":-480,"elapsed":4238,"user":{"displayName":"Brandon Chong","userId":"05776967428570980977"}},"outputId":"6b82bbfb-9af0-4e6e-a882-1f7f8f131fe5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== STEP 2: BUILDING GRAPHS ===\n","Building kNN graph (k=15) for 9999 samples...\n","Graph built: 9999 nodes, 140545 edges\n","Building kNN graph (k=15) for 22544 samples...\n","Graph built: 22544 nodes, 319048 edges\n"]}]},{"cell_type":"markdown","source":["# 5. Model Definition"],"metadata":{"id":"fJKHW5i2__-E"}},{"cell_type":"code","source":["from torch_geometric.nn import GATv2Conv\n","\n","class ContrastiveMemoryBank:\n","    def __init__(self, feature_dim, memory_size=1024, temperature=0.1):\n","        self.memory_size = memory_size\n","        self.temperature = temperature\n","        self.feature_dim = feature_dim\n","        self.memory = torch.randn(memory_size, feature_dim)\n","        self.labels = torch.randint(0, 2, (memory_size,))\n","        self.ptr = 0\n","\n","    def update(self, features, labels):\n","        batch_size = features.shape[0]\n","        if self.ptr + batch_size > self.memory_size:\n","            self.ptr = 0\n","\n","        end_ptr = min(self.ptr + batch_size, self.memory_size)\n","        actual_batch = end_ptr - self.ptr\n","\n","        self.memory[self.ptr:end_ptr] = features[:actual_batch].detach()\n","        self.labels[self.ptr:end_ptr] = labels[:actual_batch].detach()\n","        self.ptr = end_ptr % self.memory_size\n","\n","    def compute_contrastive_loss(self, features, labels, margin=0.5):\n","        if features.shape[0] == 0:\n","            return torch.tensor(0.0, device=features.device, requires_grad=True)\n","\n","        features = F.normalize(features, dim=-1)\n","        memory_features = F.normalize(self.memory, dim=-1)\n","\n","        # Move memory to same device as features\n","        if self.memory.device != features.device:\n","            self.memory = self.memory.to(features.device)\n","            self.labels = self.labels.to(features.device)\n","            memory_features = memory_features.to(features.device)\n","\n","        similarities = torch.mm(features, memory_features.T) / self.temperature\n","\n","        contrastive_loss = torch.tensor(0.0, device=features.device, requires_grad=True)\n","        for i in range(features.shape[0]):\n","            label = labels[i]\n","\n","            # Positive pairs\n","            pos_mask = (self.labels == label)\n","            if pos_mask.sum() > 0:\n","                pos_sim = similarities[i][pos_mask]\n","                pos_loss = -torch.log(torch.exp(pos_sim).sum() + 1e-8)\n","                contrastive_loss = contrastive_loss + pos_loss.mean()\n","\n","            # Negative pairs\n","            neg_mask = (self.labels != label)\n","            if neg_mask.sum() > 0:\n","                neg_sim = similarities[i][neg_mask]\n","                neg_loss = torch.clamp(margin - neg_sim, min=0).pow(2)\n","                contrastive_loss = contrastive_loss + neg_loss.mean()\n","\n","        return contrastive_loss / max(features.shape[0], 1)\n","\n","class CAGN_GAT_Fusion(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=64, output_dim=2, memory_size=1024,\n","                 feat_dropout=0.3, attn_dropout=0.3):\n","        super().__init__()\n","        self.feat_dropout = nn.Dropout(feat_dropout)\n","\n","        # CAGN branch (contrastive + attention)\n","        self.cagn_gat1 = GATv2Conv(input_dim, hidden_dim, heads=8, dropout=attn_dropout, concat=True,\n","                           edge_dim=1)\n","        self.cagn_gat2 = GATv2Conv(hidden_dim * 8, hidden_dim, heads=4, dropout=attn_dropout, concat=True,\n","                           edge_dim=1)\n","        self.cagn_gat3 = GATv2Conv(hidden_dim * 4, hidden_dim, heads=1, dropout=attn_dropout, concat=False,\n","                           edge_dim=1)\n","\n","        self.cagn_norm1 = nn.LayerNorm(hidden_dim * 8)\n","        self.cagn_norm2 = nn.LayerNorm(hidden_dim * 4)\n","        self.cagn_norm3 = nn.LayerNorm(hidden_dim)\n","\n","        # Standard GAT branch\n","        self.gat1 = GATv2Conv(input_dim, hidden_dim, heads=8, dropout=attn_dropout, concat=True,\n","                           edge_dim=1)\n","        self.gat2 = GATv2Conv(hidden_dim * 8, hidden_dim, heads=4, dropout=attn_dropout, concat=True,\n","                           edge_dim=1)\n","        self.gat3 = GATv2Conv(hidden_dim * 4, hidden_dim, heads=1, dropout=attn_dropout, concat=False,\n","                           edge_dim=1)\n","\n","        self.gat_norm1 = nn.LayerNorm(hidden_dim * 8)\n","        self.gat_norm2 = nn.LayerNorm(hidden_dim * 4)\n","        self.gat_norm3 = nn.LayerNorm(hidden_dim)\n","\n","        # Contrastive memory bank\n","        self.memory_bank = ContrastiveMemoryBank(hidden_dim, memory_size)\n","\n","        # Fusion and output\n","        self.fusion_gate = nn.Linear(2 * hidden_dim, hidden_dim)\n","        self.classifier = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x, edge_index, edge_weight=None, return_embeddings=False):\n","        # CAGN branch\n","        cagn_h = self.cagn_gat1(x, edge_index, edge_weight.unsqueeze(-1))\n","        cagn_h = self.cagn_norm1(cagn_h)\n","        cagn_h = F.elu(cagn_h)\n","        cagn_h = self.feat_dropout(cagn_h)\n","\n","        cagn_h = self.cagn_gat2(cagn_h, edge_index, edge_weight.unsqueeze(-1))\n","        cagn_h = self.cagn_norm2(cagn_h)\n","        cagn_h = F.elu(cagn_h)\n","        cagn_h = self.feat_dropout(cagn_h)\n","\n","        cagn_embeddings = self.cagn_gat3(cagn_h, edge_index, edge_weight)\n","        cagn_embeddings = self.cagn_norm3(cagn_embeddings)\n","\n","        # Standard GAT branch\n","        gat_h = self.gat1(x, edge_index, edge_weight)\n","        gat_h = self.gat_norm1(gat_h)\n","        gat_h = F.elu(gat_h)\n","        gat_h = self.feat_dropout(gat_h)\n","\n","        gat_h = self.gat2(gat_h, edge_index, edge_weight)\n","        gat_h = self.gat_norm2(gat_h)\n","        gat_h = F.elu(gat_h)\n","        gat_h = self.feat_dropout(gat_h)\n","\n","        gat_embeddings = self.gat3(gat_h, edge_index, edge_weight)\n","        gat_embeddings = self.gat_norm3(gat_embeddings)\n","\n","        # Adaptive fusion using learnable gating\n","        fused_input = torch.cat([cagn_embeddings, gat_embeddings], dim=-1)\n","        gate = torch.sigmoid(self.fusion_gate(fused_input))\n","        final_embeddings = gate * cagn_embeddings + (1 - gate) * gat_embeddings\n","\n","        if return_embeddings:\n","            return final_embeddings, cagn_embeddings\n","\n","        return self.classifier(final_embeddings)"],"metadata":{"id":"_6-Bj5Qa4Ijv","executionInfo":{"status":"ok","timestamp":1756042136414,"user_tz":-480,"elapsed":44,"user":{"displayName":"Brandon Chong","userId":"05776967428570980977"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# 6. Model Building"],"metadata":{"id":"jHcOJU0GA4Et"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"\\n=== STEP 3: MODEL SETUP ===\")\n","print(f\"Using device: {device}\")\n","\n","model = CAGN_GAT_Fusion(\n","    input_dim=24,\n","    hidden_dim=64,\n","    output_dim=2,\n","    memory_size=1024\n",").to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n","\n","def train_one_epoch(model, graph, optimizer, device, epoch, contrastive_weight=0.1, contrastive_start=10):\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    # Apply augmentation\n","    aug_graph = graph\n","\n","    # Forward pass\n","    final_embeddings, cagn_embeddings = model(\n","        aug_graph.x.to(device),\n","        aug_graph.edge_index.to(device),\n","        aug_graph.edge_attr.to(device),\n","        return_embeddings=True\n","    )\n","\n","    # Classification loss\n","    out = model.classifier(final_embeddings)\n","    clf_loss = F.cross_entropy(out, aug_graph.y.to(device))\n","\n","    # Contrastive loss (with warm-up)\n","    if epoch >= contrastive_start:\n","        contrastive_loss = model.memory_bank.compute_contrastive_loss(\n","            cagn_embeddings, aug_graph.y.to(device)\n","        )\n","        total_loss = clf_loss + contrastive_weight * contrastive_loss\n","    else:\n","        contrastive_loss = torch.tensor(0.0)\n","        total_loss = clf_loss\n","\n","    total_loss.backward()\n","    optimizer.step()\n","\n","    # Update memory bank\n","    model.memory_bank.update(cagn_embeddings.detach().cpu(), aug_graph.y.cpu())\n","\n","    return total_loss.item(), clf_loss.item(), contrastive_loss.item()\n","\n","def evaluate(model, graph, device):\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(graph.x.to(device), graph.edge_index.to(device), graph.edge_attr.to(device))\n","        preds = out.argmax(dim=-1).cpu()\n","        labels = graph.y.cpu()\n","        acc = accuracy_score(labels, preds)\n","        f1 = f1_score(labels, preds, average=\"weighted\")\n","        precision = precision_score(labels, preds, average=\"weighted\")\n","        recall = recall_score(labels, preds, average=\"weighted\")\n","    return acc, f1, precision, recall\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rS190zn25J2","executionInfo":{"status":"ok","timestamp":1756042136515,"user_tz":-480,"elapsed":97,"user":{"displayName":"Brandon Chong","userId":"05776967428570980977"}},"outputId":"5de4ca22-a96d-4f3f-fc70-a150ca4cfff2"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== STEP 3: MODEL SETUP ===\n","Using device: cpu\n"]}]},{"cell_type":"markdown","source":["# 7. Model Training"],"metadata":{"id":"SleBuIpQA-xu"}},{"cell_type":"code","source":["print(f\"\\n=== STEP 4: TRAINING ===\")\n","train_losses = []\n","clf_losses = []\n","cont_losses = []\n","test_accs = []\n","test_f1s = []\n","epochs_list = []\n","best_acc, best_f1 = 0, 0\n","best_precision, best_recall = 0, 0\n","\n","for epoch in range(1, 301):\n","    total_loss, clf_loss, cont_loss = train_one_epoch(\n","        model, train_graph, optimizer, device, epoch\n","    )\n","    acc, f1, precision, recall = evaluate(model, test_graph, device)\n","    scheduler.step()\n","\n","    train_losses.append(total_loss)\n","    clf_losses.append(clf_loss)\n","    cont_losses.append(cont_loss)\n","    test_accs.append(acc)\n","    test_f1s.append(f1)\n","    epochs_list.append(epoch)\n","\n","    if acc > best_acc:\n","        best_acc = acc\n","        best_f1 = f1\n","        best_precision = precision\n","        best_recall = recall\n","\n","    if epoch % 50 == 0 or epoch == 1:\n","        print(f\"Epoch {epoch:03d} | Total: {total_loss:.4f} | Clf: {clf_loss:.4f} | Cont: {cont_loss:.4f}\")\n","        print(f\"          | Acc: {acc:.4f} | F1: {f1:.4f} | Prec: {precision:.4f} | Rec: {recall:.4f}\")\n","\n","print(\"\\n=== FINAL RESULTS ===\")\n","print(f\"Best Accuracy:  {best_acc:.4f}\")\n","print(f\"Best F1:        {best_f1:.4f}\")\n","print(f\"Best Precision: {best_precision:.4f}\")\n","print(f\"Best Recall:    {best_recall:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wTWGvlsT43In","outputId":"986e768b-eb7e-4d5e-81e0-6ad7cd5b6d5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== STEP 4: TRAINING ===\n","Epoch 001 | Total: 0.9009 | Clf: 0.9009 | Cont: 0.0000\n","          | Acc: 0.7331 | F1: 0.7298 | Prec: 0.7894 | Rec: 0.7331\n"]}]},{"cell_type":"markdown","source":["# 8. Model Evaluation"],"metadata":{"id":"6pjXcZ7aBm26"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, classification_report\n","import seaborn as sns\n","\n","model.eval()\n","with torch.no_grad():\n","    final_out = model(test_graph.x.to(device), test_graph.edge_index.to(device), test_graph.edge_attr.to(device))\n","    final_preds = final_out.argmax(dim=-1).cpu().numpy()\n","    true_labels = test_graph.y.cpu().numpy()\n","\n","# Create plots\n","fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n","fig.suptitle('CAGN-GAT Fusion Model Analysis', fontsize=16)\n","\n","# 1. Loss curves\n","axes[0, 0].plot(epochs_list, train_losses, 'r-', label='Total Loss', linewidth=2)\n","axes[0, 0].plot(epochs_list, clf_losses, 'b-', label='Classification Loss', linewidth=2)\n","axes[0, 0].set_xlabel('Epoch')\n","axes[0, 0].set_ylabel('Loss')\n","axes[0, 0].set_title('Training Loss')\n","axes[0, 0].legend()\n","axes[0, 0].grid(True, alpha=0.3)\n","\n","# 2. Accuracy over time\n","axes[0, 1].plot(epochs_list, test_accs, 'g-', label='Test Accuracy', linewidth=2)\n","axes[0, 1].axhline(y=best_acc, color='g', linestyle='--', alpha=0.7, label=f'Best: {best_acc:.3f}')\n","axes[0, 1].set_xlabel('Epoch')\n","axes[0, 1].set_ylabel('Accuracy')\n","axes[0, 1].set_title('Accuracy Evolution')\n","axes[0, 1].legend()\n","axes[0, 1].grid(True, alpha=0.3)\n","\n","# 3. Confusion Matrix\n","cm = confusion_matrix(true_labels, final_preds)\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],\n","            xticklabels=['Normal', 'Attack'], yticklabels=['Normal', 'Attack'])\n","axes[1, 0].set_xlabel('Predicted')\n","axes[1, 0].set_ylabel('Actual')\n","axes[1, 0].set_title('Confusion Matrix')\n","\n","# 4. Contrastive Loss\n","axes[1, 1].plot(epochs_list, cont_losses, 'purple', linewidth=2)\n","axes[1, 1].axvline(x=10, color='red', linestyle='--', alpha=0.7, label='Contrastive Start')\n","axes[1, 1].set_xlabel('Epoch')\n","axes[1, 1].set_ylabel('Contrastive Loss')\n","axes[1, 1].set_title('Contrastive Learning')\n","axes[1, 1].legend()\n","axes[1, 1].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"\\nFinal Results: Accuracy={best_acc:.4f}, F1={best_f1:.4f}\")\n","print(\"\\nClassification Report:\")\n","print(classification_report(true_labels, final_preds, target_names=['Normal', 'Attack']))"],"metadata":{"id":"vIjokfJT_jsh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/My Drive/Colab Notebooks/Intrusion-detection-system-dli\n","\n","# Copy the Mateen folder and its contents into the current directory\n","!cp -r '/content/drive/My Drive/Colab Notebooks/DLI' .\n","# Add the new folder and its contents to the staging area\n","!git add DLI\n","\n","# Commit the changes\n","!git commit -m \"Added DLI project as a subfolder\"\n","\n","# Push the changes to GitHub\n","!git push"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XolWvTZwTFDO","executionInfo":{"status":"ok","timestamp":1756043078814,"user_tz":-480,"elapsed":2119,"user":{"displayName":"Brandon Chong","userId":"05776967428570980977"}},"outputId":"dcc3f78c-b202-4eaf-936a-64dc8be239d9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks/Intrusion-detection-system-dli\n","[main e5bc895] Added DLI project as a subfolder\n"," 5 files changed, 149162 insertions(+)\n"," create mode 100644 DLI/DLI Assignment.ipynb\n"," create mode 100644 DLI/KDDTest+.txt\n"," create mode 100644 DLI/KDDTrain+.txt\n"," create mode 100644 DLI/__pycache__/nsl_preprocessing.cpython-312.pyc\n"," create mode 100644 DLI/nsl_preprocessing.py\n","remote: Invalid username or token. Password authentication is not supported for Git operations.\n","fatal: Authentication failed for 'https://github.com/Micro-Maxis/Intrusion-detection-system-dli.git/'\n"]}]},{"cell_type":"code","source":["import os\n","\n","# ðŸ”´ Replace with your NEW token (not the leaked one)\n","os.environ[\"GITHUB_TOKEN\"] = \"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\"\n","\n","# Now clone with token\n","!git remote set-url origin https://$GITHUB_TOKEN@github.com/Micro-Maxis/Intrusion-detection-system-dli.git\n"],"metadata":{"id":"Dm5e76N5UiY9","executionInfo":{"status":"ok","timestamp":1756043268653,"user_tz":-480,"elapsed":132,"user":{"displayName":"Brandon Chong","userId":"05776967428570980977"}}},"execution_count":6,"outputs":[]}]}