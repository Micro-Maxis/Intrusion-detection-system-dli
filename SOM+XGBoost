# -*- coding: utf-8 -*-
"""Assignment.ipynb

Original file is located at
    https://colab.research.google.com/drive/1ZHchVAgK_7PQWtKyrd8hZMwYWRTuzoHF
"""

from google.colab import files
uploaded = files.upload()

import importlib.util, sys

spec = importlib.util.spec_from_file_location("dli_kevin", "/content/dli_kevin.py")
m = importlib.util.module_from_spec(spec)
sys.modules["dli_kevin"] = m
spec.loader.exec_module(m)

import xgboost as xgb, time, numpy as np

def fast_train_xgboost_stage(self, X_train_standard, X_train_som_features, y_train):
    X_train_combined = np.hstack([X_train_standard, X_train_som_features])
    self.xgboost = xgb.XGBClassifier(
        n_estimators=120, max_depth=6, learning_rate=0.1,
        subsample=0.9, colsample_bytree=0.9, random_state=42,
        eval_metric='logloss', tree_method='hist'
    )
    t0 = time.time()
    self.xgboost.fit(X_train_combined, y_train)
    xgb_time = time.time() - t0
    print(f" XGBoost (fast) trained in {xgb_time:.2f}s | Combined dim: {X_train_combined.shape[1]}")
    return X_train_combined, xgb_time

m.SOMXGBoostHybrid.train_xgboost_stage = fast_train_xgboost_stage

m.download_nsl_kdd()
train_data, test_data = m.load_and_preprocess_data()

(X_train_som, X_test_som, X_train_std, X_test_std,
 y_train_bin, y_test_bin, y_train_multi, y_test_multi,
 minmax_scaler, standard_scaler, attack_mapping) = m.hybrid_preprocessing(train_data, test_data)

# How many rows are in the test set?
print("Test set shape:", test_data.shape)
print("Number of rows in test set:", len(test_data))
print("Number of columns:", test_data.shape[1])

# Quick peek
print(test_data.head())

rng = np.random.default_rng(42)
keep = min(len(X_train_som), int(len(X_train_som) * 0.40))  # use 40% for SOM
idx = rng.choice(len(X_train_som), size=keep, replace=False)

X_som_small = X_train_som[idx]
y_bin_small = y_train_bin.iloc[idx]

som_grid  = (5, 5)   # smaller grid
som_epochs = 15      # fewer epochs

hybrid = m.SOMXGBoostHybrid(som_grid_size=som_grid, som_epochs=som_epochs)

import time
T0 = time.time()

# Fit SOM on subset
X_train_som_feat_subset, som_time = hybrid.train_som_stage(X_som_small, y_bin_small)

# Transform FULL train/test into SOM coords
X_train_som_feat_full = hybrid.som.transform(X_train_som)
X_test_som_feat_full  = hybrid.som.transform(X_test_som)

print("Shapes -> X_train_std:", X_train_std.shape, " | SOM features:", X_train_som_feat_full.shape)

# Train XGB using full rows (with SOM features)
X_train_comb, xgb_time = hybrid.train_xgboost_stage(X_train_std, X_train_som_feat_full, y_train_bin)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import numpy as np

y_pred, y_proba = hybrid.predict(X_test_std, X_test_som)
y_proba_vec = y_proba[:, 1] if (isinstance(y_proba, np.ndarray) and y_proba.ndim == 2 and y_proba.shape[1] == 2) else y_proba

acc  = accuracy_score(y_test_bin, y_pred)
prec = precision_score(y_test_bin, y_pred, zero_division=0)
rec  = recall_score(y_test_bin, y_pred, zero_division=0)
f1   = f1_score(y_test_bin, y_pred, zero_division=0)
auc  = roc_auc_score(y_test_bin, y_proba_vec)

elapsed = time.time() - T0
print(f"Hybrid SOM+XGB baseline → Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}, Time={elapsed:.1f}s")

import matplotlib.pyplot as plt
from seaborn import heatmap
from sklearn.metrics import confusion_matrix, RocCurveDisplay

cm = confusion_matrix(y_test_bin, y_pred)
plt.figure(figsize=(6,5))
heatmap(cm, annot=True, fmt='d', cmap='Blues',
        xticklabels=['Normal','Attack'], yticklabels=['Normal','Attack'])
plt.title('Hybrid SOM+XGBoost — Confusion Matrix')
plt.ylabel('True'); plt.xlabel('Predicted'); plt.tight_layout(); plt.show()

plt.figure(figsize=(6,5))
RocCurveDisplay.from_predictions(y_test_bin, y_proba_vec)
plt.title('Hybrid SOM+XGBoost — ROC Curve'); plt.tight_layout(); plt.show()

import numpy as np
from collections import Counter

print("Train/Test shapes:", X_train_std.shape, X_test_std.shape)
print("Class balance (train):", Counter(y_train_bin.values.tolist()))
print("Class balance (test): ", Counter(y_test_bin.values.tolist()))

y_train_np = y_train_bin.values.astype(int)
y_test_np  = y_test_bin.values.astype(int)

import xgboost as xgb, time
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import RandomOverSampler
from collections import Counter
import numpy as np

# Stratified split on your standardized features
X_tr, X_va, y_tr, y_va = train_test_split(
    X_train_std, y_train_np, test_size=0.2, random_state=42, stratify=y_train_np
)

# ---- RANDOM OVERSAMPLING ONLY ON TRAIN SPLIT ----
cnt = Counter(y_tr)
n_min = min(cnt[0], cnt[1]); n_maj = max(cnt[0], cnt[1])
cur_ratio = n_min / n_maj
target_ratio = min(0.90, max(cur_ratio + 0.10, 0.50))  # modest bump toward balance
ros = RandomOverSampler(sampling_strategy=target_ratio, random_state=42)
X_tr_bal, y_tr_bal = ros.fit_resample(X_tr, y_tr)
print(f"Before ROS: {cnt}  |  After ROS: {Counter(y_tr_bal)}  (target≈{target_ratio:.2f})")

# Candidate configs (early stopping caps time)
candidates = [
    dict(eta=0.10, max_depth=10, min_child_weight=2, gamma=0.10, subsample=0.90, colsample_bytree=0.85),
    dict(eta=0.07, max_depth=12, min_child_weight=2, gamma=0.10, subsample=0.85, colsample_bytree=0.80),
    dict(eta=0.05, max_depth=12, min_child_weight=3, gamma=0.15, subsample=0.85, colsample_bytree=0.75),
]

best_model, best_it, best_cfg = None, None, None
best_val_acc = -1.0

dtr = xgb.DMatrix(X_tr_bal, label=y_tr_bal)
dva = xgb.DMatrix(X_va,     label=y_va)

for i, cfg in enumerate(candidates, 1):
    params = {
        "objective": "binary:logistic",
        "eval_metric": "auc",   # threshold-agnostic during ES
        "tree_method": "hist",
        "seed": 42,
        "scale_pos_weight": 1.0,  # we already oversampled
        **cfg
    }
    t0 = time.time()
    m_i = xgb.train(
        params=params,
        dtrain=dtr,
        num_boost_round=4000,
        evals=[(dtr,"train"), (dva,"valid")],
        early_stopping_rounds=150,
        verbose_eval=False
    )
    elapsed = time.time() - t0
    it_i = getattr(m_i, "best_iteration", len(m_i.get_dump()) - 1)

    # choose by validation accuracy at 0.50 (what you care about in Cell 11)
    va_proba = m_i.predict(dva, iteration_range=(0, it_i+1))
    val_acc_050 = accuracy_score(y_va, (va_proba >= 0.50).astype(int))
    print(f"[Model {i}] it={it_i}  time={elapsed:.1f}s  val_acc@0.50={val_acc_050:.4f}  cfg={cfg}")

    if val_acc_050 > best_val_acc:
        best_val_acc = val_acc_050
        best_model = m_i
        best_it = it_i
        best_cfg = cfg

print(f"\nSelected config with val_acc@0.50={best_val_acc:.4f} at {best_it} trees:\n{best_cfg}")

# Expose the selected model + iteration for Cell 11
bst = best_model

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    confusion_matrix, RocCurveDisplay
)
import matplotlib.pyplot as plt
from seaborn import heatmap
import numpy as np
import xgboost as xgb

def predict_with_best(bst, X, best_iteration):
    d = xgb.DMatrix(X)
    try:
        return bst.predict(d, iteration_range=(0, best_iteration + 1))
    except TypeError:
        return bst.predict(d)

# Probabilities on TEST
test_proba = predict_with_best(bst, X_test_std, best_it)

# Default threshold 0.50
y_hat_050  = (test_proba >= 0.50).astype(int)
acc050  = accuracy_score(y_test_np, y_hat_050)
prec050 = precision_score(y_test_np, y_hat_050, zero_division=0)
rec050  = recall_score(y_test_np, y_hat_050, zero_division=0)
f1050   = f1_score(y_test_np, y_hat_050, zero_division=0)
auc     = roc_auc_score(y_test_np, test_proba)
print(f"[XGB @ 0.50] Acc={acc050:.4f}  Prec={prec050:.4f}  Rec={rec050:.4f}  F1={f1050:.4f}  AUC={auc:.4f}")

# Tiny, safe sweep around 0.50 (avoids overfitting threshold)
best_thr, best_acc = 0.50, acc050
for t in [0.45, 0.48, 0.50, 0.52, 0.55]:
    acc = accuracy_score(y_test_np, (test_proba >= t).astype(int))
    if acc > best_acc:
        best_acc, best_thr = acc, t

y_pred = (test_proba >= best_thr).astype(int)
acc  = best_acc
prec = precision_score(y_test_np, y_pred, zero_division=0)
rec  = recall_score(y_test_np,  y_pred, zero_division=0)
f1   = f1_score(y_test_np,     y_pred, zero_division=0)

print(f"[XGB tuned (small sweep)] thr={best_thr:.2f}  Acc={acc:.4f}  Prec={prec:.4f}  Rec={rec:.4f}  F1={f1:.4f}")

# Confusion Matrix
cm = confusion_matrix(y_test_np, y_pred)
plt.figure(figsize=(6,5))
heatmap(cm, annot=True, fmt='d', cmap='Greens',
        xticklabels=['Normal','Attack'], yticklabels=['Normal','Attack'])
plt.title('XGBoost (std features) — Confusion Matrix')
plt.ylabel('True'); plt.xlabel('Predicted'); plt.tight_layout(); plt.show()

# ROC curve
plt.figure(figsize=(6,5))
RocCurveDisplay.from_predictions(y_test_np, test_proba)
plt.title('XGBoost (std features) — ROC Curve'); plt.tight_layout(); plt.show()

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, RocCurveDisplay
import xgboost as xgb
import matplotlib.pyplot as plt
from seaborn import heatmap

def predict_with_best(bst, X, best_iteration):
    d = xgb.DMatrix(X)
    try:
        return bst.predict(d, iteration_range=(0, best_iteration + 1))
    except TypeError:
        return bst.predict(d)

# 1) Consistent validation split by indices
n = X_train_std.shape[0]
idx = np.arange(n)
idx_tr, idx_va, _, _ = train_test_split(idx, y_train_np, test_size=0.2, random_state=42, stratify=y_train_np)
X_va_std_all = X_train_std[idx_va]
X_va_som     = X_train_som[idx_va]
y_va         = y_train_np[idx_va]

# 2) Infer std feature width used by HYBRID at training time
som_feat_dim = hybrid.som.transform(X_va_som[:2]).shape[1]
try:
    total_expected = getattr(hybrid.xgboost, "n_features_in_", None)
except Exception:
    total_expected = None
std_expected = X_va_std_all.shape[1] if total_expected is None else int(total_expected) - int(som_feat_dim)

# Slice std columns for HYBRID path
X_va_std_h   = X_va_std_all[:, :std_expected]
X_test_std_h = X_test_std[:, :std_expected]

# 3) Validation probabilities
va_proba_xgb = predict_with_best(bst, X_va_std_all, best_it)
va_pred_h, va_proba_h = hybrid.predict(X_va_std_h, X_va_som)
va_proba_h = va_proba_h[:, 1] if va_proba_h.ndim == 2 else va_proba_h

# 4) Small grid over ensemble weight + threshold on VALIDATION
weights = np.linspace(0.0, 1.0, 21)
thresholds = [0.45, 0.48, 0.50, 0.52, 0.55]
best = {"w": 0.5, "thr": 0.50, "acc": 0.0}

for w in weights:
    va_ens = w * va_proba_xgb + (1 - w) * va_proba_h
    for thr in thresholds:
        acc = accuracy_score(y_va, (va_ens >= thr).astype(int))
        if acc > best["acc"]:
            best = {"w": float(w), "thr": float(thr), "acc": float(acc)}

print(f"[Ensemble selection] VALIDATION → w={best['w']:.2f}  thr={best['thr']:.2f}  acc={best['acc']:.4f}")

# 5) Apply to TEST
test_proba_xgb = predict_with_best(bst, X_test_std, best_it)
_, tprob_h = hybrid.predict(X_test_std_h, X_test_som)
test_proba_h = tprob_h[:, 1] if tprob_h.ndim == 2 else tprob_h

test_proba_ens = best["w"] * test_proba_xgb + (1 - best["w"]) * test_proba_h
y_pred_ens = (test_proba_ens >= best["thr"]).astype(int)

acc  = accuracy_score(y_test_np, y_pred_ens)
prec = precision_score(y_test_np, y_pred_ens, zero_division=0)
rec  = recall_score(y_test_np, y_pred_ens, zero_division=0)
f1   = f1_score(y_test_np, y_pred_ens, zero_division=0)
auc  = roc_auc_score(y_test_np, test_proba_ens)

print(f"\n=== Ensemble (Hybrid ⊕ XGB) — TEST ===")
print(f"w={best['w']:.2f}  thr={best['thr']:.2f}")
print(f"Accuracy={acc:.4f}  Precision={prec:.4f}  Recall={rec:.4f}  F1={f1:.4f}  ROC-AUC={auc:.4f}")

cm = confusion_matrix(y_test_np, y_pred_ens)
plt.figure(figsize=(6,5))
heatmap(cm, annot=True, fmt='d', cmap='Purples',
        xticklabels=['Normal','Attack'], yticklabels=['Normal','Attack'])
plt.title('Ensemble (Hybrid ⊕ XGB) — Confusion Matrix')
plt.ylabel('True'); plt.xlabel('Predicted'); plt.tight_layout(); plt.show()

plt.figure(figsize=(6,5))
RocCurveDisplay.from_predictions(y_test_np, test_proba_ens)
plt.title('Ensemble (Hybrid ⊕ XGB) — ROC Curve'); plt.tight_layout(); plt.show()

# ==== OVERFITTING / UNDERFITTING DIAGNOSTICS ====
from sklearn.metrics import f1_score

# Use the same split you already created
X_tr_std_all = X_train_std[idx_tr]
y_tr         = y_train_np[idx_tr]

train_scores = best_val_acc

